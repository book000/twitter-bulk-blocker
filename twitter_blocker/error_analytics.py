"""
HTTPã‚¨ãƒ©ãƒ¼çµ±è¨ˆåé›†ãƒ»åˆ†æã‚·ã‚¹ãƒ†ãƒ 
é•·æœŸç¨¼åƒæ™‚ã®ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³è©³ç´°åˆ†ææ©Ÿèƒ½
"""

import json
import sqlite3
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from .database import DatabaseManager


class HTTPErrorAnalytics:
    """HTTP ã‚¨ãƒ©ãƒ¼çµ±è¨ˆåé›†ãƒ»åˆ†æã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, database_manager: DatabaseManager):
        self.db = database_manager
        self.error_timeline = []  # (timestamp, error_type, context)
        self.session_id = self._generate_session_id()
        self.init_analytics_tables()
    
    def _generate_session_id(self) -> str:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’ç”Ÿæˆ"""
        return f"session_{int(time.time())}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    def init_analytics_tables(self):
        """ã‚¨ãƒ©ãƒ¼åˆ†æç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®åˆæœŸåŒ–"""
        with sqlite3.connect(self.db.db_file) as conn:
            cursor = conn.cursor()
            
            # HTTP ã‚¨ãƒ©ãƒ¼è©³ç´°ãƒ­ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS http_error_analytics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp REAL NOT NULL,
                    session_id TEXT NOT NULL,
                    error_type TEXT NOT NULL,
                    status_code INTEGER,
                    response_text TEXT,
                    headers_json TEXT,
                    runtime_hours REAL,
                    retry_count INTEGER,
                    success_rate_before REAL,
                    header_enhancement_active BOOLEAN,
                    user_context TEXT,
                    recovery_time_seconds REAL,
                    container_name TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # é•·æœŸãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS long_term_patterns (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_date DATE NOT NULL,
                    session_id TEXT NOT NULL,
                    total_runtime_hours REAL,
                    error_type_distribution TEXT,  -- JSON
                    critical_transitions TEXT,     -- JSON 
                    effectiveness_metrics TEXT,    -- JSON
                    recommendations TEXT,          -- JSON
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # æ™‚é–“å¸¯åˆ¥ã‚¨ãƒ©ãƒ¼çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS hourly_error_stats (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id TEXT NOT NULL,
                    hour_offset INTEGER NOT NULL,  -- ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹ã‹ã‚‰ã®æ™‚é–“ï¼ˆæ™‚ï¼‰
                    total_requests INTEGER DEFAULT 0,
                    total_errors INTEGER DEFAULT 0,
                    error_types_json TEXT,  -- JSON: {error_type: count}
                    success_rate REAL DEFAULT 1.0,
                    avg_response_time REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(session_id, hour_offset)
                )
            """)
            
            conn.commit()
            print(f"ğŸ“Š HTTPã‚¨ãƒ©ãƒ¼åˆ†æãƒ†ãƒ¼ãƒ–ãƒ«åˆæœŸåŒ–å®Œäº†: {self.db.db_file}")
    
    def record_error_with_context(self, error_data: Dict[str, Any]) -> None:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ãã‚¨ãƒ©ãƒ¼è¨˜éŒ²"""
        with sqlite3.connect(self.db.db_file) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT INTO http_error_analytics 
                (timestamp, session_id, error_type, status_code, response_text, 
                 headers_json, runtime_hours, retry_count, success_rate_before,
                 header_enhancement_active, user_context, recovery_time_seconds, container_name)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                error_data['timestamp'],
                self.session_id,
                error_data['error_type'],
                error_data['status_code'],
                error_data.get('response_text', ''),
                json.dumps(error_data.get('headers', {})),
                error_data['runtime_hours'],
                error_data['retry_count'],
                error_data['success_rate_before'],
                error_data['header_enhancement_active'],
                error_data.get('user_context', ''),
                error_data.get('recovery_time_seconds'),
                error_data.get('container_name', 'unknown')
            ))
    
    def update_hourly_stats(self, runtime_hours: float, error_occurred: bool, 
                          error_type: str = None, response_time: float = None) -> None:
        """æ™‚é–“å¸¯åˆ¥çµ±è¨ˆã®æ›´æ–°"""
        hour_offset = int(runtime_hours)
        
        with sqlite3.connect(self.db.db_file) as conn:
            cursor = conn.cursor()
            
            # æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—ã¾ãŸã¯æ–°è¦ä½œæˆ
            cursor.execute("""
                SELECT total_requests, total_errors, error_types_json, success_rate
                FROM hourly_error_stats 
                WHERE session_id = ? AND hour_offset = ?
            """, (self.session_id, hour_offset))
            
            result = cursor.fetchone()
            
            if result:
                total_requests, total_errors, error_types_json, current_success_rate = result
                error_types = json.loads(error_types_json) if error_types_json else {}
            else:
                total_requests, total_errors = 0, 0
                error_types = {}
                current_success_rate = 1.0
            
            # çµ±è¨ˆæ›´æ–°
            total_requests += 1
            if error_occurred:
                total_errors += 1
                if error_type:
                    error_types[error_type] = error_types.get(error_type, 0) + 1
            
            new_success_rate = (total_requests - total_errors) / total_requests if total_requests > 0 else 1.0
            
            # ãƒ¬ã‚³ãƒ¼ãƒ‰æ›´æ–°ã¾ãŸã¯æŒ¿å…¥
            cursor.execute("""
                INSERT OR REPLACE INTO hourly_error_stats 
                (session_id, hour_offset, total_requests, total_errors, 
                 error_types_json, success_rate, avg_response_time)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                self.session_id,
                hour_offset,
                total_requests,
                total_errors,
                json.dumps(error_types),
                new_success_rate,
                response_time
            ))
    
    def analyze_error_progression_patterns(self) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼é€²è¡Œãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        with sqlite3.connect(self.db.db_file) as conn:
            cursor = conn.cursor()
            
            # æ™‚é–“åˆ¥ã‚¨ãƒ©ãƒ¼æ¨ç§»ã‚’å–å¾—
            cursor.execute("""
                SELECT hour_offset, total_requests, total_errors, success_rate, error_types_json
                FROM hourly_error_stats 
                WHERE session_id = ?
                ORDER BY hour_offset
            """, (self.session_id,))
            
            hourly_data = cursor.fetchall()
            
            if not hourly_data:
                return {"status": "no_data", "message": "åˆ†æã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"}
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            patterns = {
                "steady_decline": False,    # æ®µéšçš„æ‚ªåŒ–
                "sudden_spike": False,      # æ€¥æ¿€ãªã‚¨ãƒ©ãƒ¼å¢—åŠ 
                "periodic_issues": False,   # å‘¨æœŸçš„ãªå•é¡Œ
                "recovery_pattern": False,  # å›å¾©ãƒ‘ã‚¿ãƒ¼ãƒ³
                "critical_threshold": False # é‡è¦é–¾å€¤åˆ°é”
            }
            
            success_rates = [row[3] for row in hourly_data]
            error_counts = [row[2] for row in hourly_data]
            
            # æ®µéšçš„æ‚ªåŒ–ã®æ¤œå‡º
            if len(success_rates) >= 3:
                declining_trend = all(
                    success_rates[i] >= success_rates[i+1] 
                    for i in range(len(success_rates)-1)
                )
                if declining_trend and success_rates[-1] < 0.8:
                    patterns["steady_decline"] = True
            
            # æ€¥æ¿€ãªã‚¨ãƒ©ãƒ¼å¢—åŠ ã®æ¤œå‡º
            if len(error_counts) >= 2:
                for i in range(1, len(error_counts)):
                    if error_counts[i] > error_counts[i-1] * 3:  # 3å€ä»¥ä¸Šã®å¢—åŠ 
                        patterns["sudden_spike"] = True
                        break
            
            # é‡è¦é–¾å€¤åˆ°é”ã®æ¤œå‡º
            if success_rates and success_rates[-1] < 0.5:  # æˆåŠŸç‡50%æœªæº€
                patterns["critical_threshold"] = True
            
            return {
                "status": "analysis_complete",
                "patterns_detected": patterns,
                "hourly_data": hourly_data,
                "recommendations": self._generate_pattern_recommendations(patterns)
            }
    
    def _generate_pattern_recommendations(self, patterns: Dict[str, bool]) -> List[str]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãæ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []
        
        if patterns["steady_decline"]:
            recommendations.append("ğŸ”„ æ®µéšçš„æ‚ªåŒ–æ¤œå‡º - Cookieäºˆé˜²çš„å†èª­ã¿è¾¼ã¿ã‚’æ¨å¥¨")
            recommendations.append("ğŸ“‰ ãƒ˜ãƒƒãƒ€ãƒ¼æˆ¦ç•¥ã®æ®µéšçš„å¤‰æ›´ã‚’æ¤œè¨")
        
        if patterns["sudden_spike"]:
            recommendations.append("ğŸš¨ æ€¥æ¿€ãªã‚¨ãƒ©ãƒ¼å¢—åŠ  - å³åº§ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆåœæ­¢ã—ã¦åŸå› èª¿æŸ»")
            recommendations.append("ğŸ”§ ã‚¢ãƒ³ãƒãƒœãƒƒãƒˆæ¤œå‡ºã®å¯èƒ½æ€§ - ãƒ˜ãƒƒãƒ€ãƒ¼å³åº§å¤‰æ›´")
        
        if patterns["critical_threshold"]:
            recommendations.append("âš ï¸ é‡è¦é–¾å€¤åˆ°é” - ã‚·ã‚¹ãƒ†ãƒ åœæ­¢ã‚’æ¤œè¨")
            recommendations.append("ğŸ¥ ç·Šæ€¥ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ãƒ¢ãƒ¼ãƒ‰ã¸ã®åˆ‡ã‚Šæ›¿ãˆ")
        
        if not any(patterns.values()):
            recommendations.append("âœ… å¥å…¨ãªãƒ‘ã‚¿ãƒ¼ãƒ³ - ç¾åœ¨ã®æˆ¦ç•¥ç¶™ç¶š")
        
        return recommendations
    
    def generate_weekly_analysis_report(self) -> Dict[str, Any]:
        """é€±æ¬¡åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        with sqlite3.connect(self.db.db_file) as conn:
            cursor = conn.cursor()
            
            # éå»7æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ
            cursor.execute("""
                SELECT error_type, runtime_hours, COUNT(*) as frequency,
                       AVG(recovery_time_seconds) as avg_recovery,
                       header_enhancement_active
                FROM http_error_analytics 
                WHERE timestamp > ? 
                GROUP BY error_type, ROUND(runtime_hours), header_enhancement_active
                ORDER BY runtime_hours, frequency DESC
            """, (time.time() - 7*24*3600,))
            
            results = cursor.fetchall()
            
            return {
                "analysis_period": "7_days",
                "error_patterns": self._analyze_error_patterns(results),
                "header_effectiveness": self._analyze_header_effectiveness(results),
                "runtime_correlations": self._analyze_runtime_correlations(results),
                "optimization_recommendations": self._generate_optimization_recommendations(results)
            }
    
    def _analyze_error_patterns(self, results: List[Tuple]) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        error_by_type = {}
        error_by_hour = {}
        
        for error_type, runtime_hours, frequency, avg_recovery, header_enhanced in results:
            # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
            if error_type not in error_by_type:
                error_by_type[error_type] = {
                    "total_frequency": 0,
                    "avg_recovery_time": 0,
                    "first_occurrence_hour": runtime_hours,
                    "header_effectiveness": {"with": 0, "without": 0}
                }
            
            error_by_type[error_type]["total_frequency"] += frequency
            error_by_type[error_type]["avg_recovery_time"] = avg_recovery or 0
            
            if header_enhanced:
                error_by_type[error_type]["header_effectiveness"]["with"] += frequency
            else:
                error_by_type[error_type]["header_effectiveness"]["without"] += frequency
            
            # æ™‚é–“åˆ¥é›†è¨ˆ
            hour_key = f"{int(runtime_hours)}h"
            if hour_key not in error_by_hour:
                error_by_hour[hour_key] = {"total_errors": 0, "error_types": {}}
            
            error_by_hour[hour_key]["total_errors"] += frequency
            error_by_hour[hour_key]["error_types"][error_type] = frequency
        
        return {
            "by_error_type": error_by_type,
            "by_runtime_hour": error_by_hour,
            "dominant_error_types": sorted(
                error_by_type.items(), 
                key=lambda x: x[1]["total_frequency"], 
                reverse=True
            )[:5]
        }
    
    def _analyze_header_effectiveness(self, results: List[Tuple]) -> Dict[str, Any]:
        """ãƒ˜ãƒƒãƒ€ãƒ¼æœ‰åŠ¹æ€§ã®åˆ†æ"""
        with_headers = {"total_errors": 0, "error_types": {}}
        without_headers = {"total_errors": 0, "error_types": {}}
        
        for error_type, runtime_hours, frequency, avg_recovery, header_enhanced in results:
            if header_enhanced:
                with_headers["total_errors"] += frequency
                with_headers["error_types"][error_type] = with_headers["error_types"].get(error_type, 0) + frequency
            else:
                without_headers["total_errors"] += frequency
                without_headers["error_types"][error_type] = without_headers["error_types"].get(error_type, 0) + frequency
        
        # åŠ¹æœæ€§ã®è¨ˆç®—
        total_with = with_headers["total_errors"]
        total_without = without_headers["total_errors"]
        total_requests = total_with + total_without
        
        effectiveness_score = 0.0
        if total_requests > 0:
            error_rate_with = total_with / total_requests
            error_rate_without = total_without / total_requests
            
            if total_without > 0:
                effectiveness_score = max(0, (error_rate_without - error_rate_with) / error_rate_without)
        
        return {
            "with_headers": with_headers,
            "without_headers": without_headers,
            "effectiveness_score": effectiveness_score,
            "recommendation": "use_enhanced" if effectiveness_score > 0.2 else "use_basic"
        }
    
    def _analyze_runtime_correlations(self, results: List[Tuple]) -> Dict[str, Any]:
        """ç¨¼åƒæ™‚é–“ã¨ã‚¨ãƒ©ãƒ¼ã®ç›¸é–¢åˆ†æ"""
        runtime_buckets = {
            "0-1h": {"errors": 0, "types": {}},
            "1-3h": {"errors": 0, "types": {}},
            "3-6h": {"errors": 0, "types": {}},
            "6-12h": {"errors": 0, "types": {}},
            "12h+": {"errors": 0, "types": {}}
        }
        
        for error_type, runtime_hours, frequency, avg_recovery, header_enhanced in results:
            bucket = self._get_runtime_bucket(runtime_hours)
            runtime_buckets[bucket]["errors"] += frequency
            runtime_buckets[bucket]["types"][error_type] = runtime_buckets[bucket]["types"].get(error_type, 0) + frequency
        
        # ç›¸é–¢åˆ†æ
        correlations = {
            "error_increase_over_time": False,
            "critical_hours": [],
            "stable_periods": []
        }
        
        # ã‚¨ãƒ©ãƒ¼å¢—åŠ å‚¾å‘ã®æ¤œå‡º
        error_counts = [bucket["errors"] for bucket in runtime_buckets.values()]
        if len(error_counts) >= 3:
            increasing_trend = sum(
                1 for i in range(len(error_counts)-1) 
                if error_counts[i+1] > error_counts[i]
            ) > len(error_counts) // 2
            correlations["error_increase_over_time"] = increasing_trend
        
        # é‡è¦æ™‚é–“å¸¯ã®ç‰¹å®š
        max_errors = max(bucket["errors"] for bucket in runtime_buckets.values())
        if max_errors > 0:
            for bucket_name, bucket_data in runtime_buckets.items():
                if bucket_data["errors"] > max_errors * 0.7:  # æœ€å¤§ã‚¨ãƒ©ãƒ¼æ•°ã®70%ä»¥ä¸Š
                    correlations["critical_hours"].append(bucket_name)
        
        return {
            "runtime_buckets": runtime_buckets,
            "correlations": correlations,
            "insights": self._generate_runtime_insights(runtime_buckets, correlations)
        }
    
    def _get_runtime_bucket(self, runtime_hours: float) -> str:
        """ç¨¼åƒæ™‚é–“ã‚’ãƒã‚±ãƒƒãƒˆã«åˆ†é¡"""
        if runtime_hours < 1:
            return "0-1h"
        elif runtime_hours < 3:
            return "1-3h"
        elif runtime_hours < 6:
            return "3-6h"
        elif runtime_hours < 12:
            return "6-12h"
        else:
            return "12h+"
    
    def _generate_runtime_insights(self, runtime_buckets: Dict, correlations: Dict) -> List[str]:
        """ç¨¼åƒæ™‚é–“åˆ†æã‹ã‚‰ã®æ´å¯Ÿç”Ÿæˆ"""
        insights = []
        
        if correlations["error_increase_over_time"]:
            insights.append("âš ï¸ ç¨¼åƒæ™‚é–“ã®å¢—åŠ ã«ä¼´ã„ã‚¨ãƒ©ãƒ¼ãŒå¢—åŠ ã™ã‚‹å‚¾å‘")
            insights.append("ğŸ”„ å®šæœŸçš„ãªã‚·ã‚¹ãƒ†ãƒ ãƒªã‚»ãƒƒãƒˆã®æ¤œè¨ãŒå¿…è¦")
        
        if "3-6h" in correlations["critical_hours"]:
            insights.append("ğŸ•’ 3-6æ™‚é–“ãŒæœ€ã‚‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã‚„ã™ã„æ™‚é–“å¸¯")
            insights.append("ğŸ›¡ï¸ ã“ã®æ™‚é–“å¸¯ã§ã®äºˆé˜²çš„å¯¾ç­–ã®å¼·åŒ–ã‚’æ¨å¥¨")
        
        if not correlations["critical_hours"]:
            insights.append("âœ… ç‰¹å®šã®å•é¡Œæ™‚é–“å¸¯ãªã— - å®‰å®šã—ãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
        
        return insights
    
    def _generate_optimization_recommendations(self, results: List[Tuple]) -> List[str]:
        """æœ€é©åŒ–æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []
        
        # ã‚¨ãƒ©ãƒ¼é »åº¦åˆ†æ
        total_errors = sum(frequency for _, _, frequency, _, _ in results)
        
        if total_errors > 100:
            recommendations.append("ğŸš¨ é«˜é »åº¦ã‚¨ãƒ©ãƒ¼æ¤œå‡º - ç·Šæ€¥å¯¾å¿œãŒå¿…è¦")
            recommendations.append("ğŸ”§ åŸºæœ¬è¨­å®šã®è¦‹ç›´ã—ã‚’æ¨å¥¨")
        elif total_errors > 50:
            recommendations.append("âš ï¸ ä¸­ç¨‹åº¦ã®ã‚¨ãƒ©ãƒ¼ - äºˆé˜²çš„å¯¾ç­–ã®å¼·åŒ–")
        else:
            recommendations.append("âœ… ä½ã‚¨ãƒ©ãƒ¼ç‡ - ç¾åœ¨ã®è¨­å®šç¶™ç¶š")
        
        # å¾©æ—§æ™‚é–“åˆ†æ
        recovery_times = [avg_recovery for _, _, _, avg_recovery, _ in results if avg_recovery]
        if recovery_times:
            avg_recovery = sum(recovery_times) / len(recovery_times)
            if avg_recovery > 300:  # 5åˆ†ä»¥ä¸Š
                recommendations.append("â±ï¸ å¾©æ—§æ™‚é–“ãŒé•·ã„ - ãƒãƒƒã‚¯ã‚ªãƒ•æˆ¦ç•¥ã®èª¿æ•´ãŒå¿…è¦")
        
        return recommendations
    
    def get_real_time_status(self) -> Dict[str, Any]:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çŠ¶æ…‹ã®å–å¾—"""
        current_time = time.time()
        
        with sqlite3.connect(self.db.db_file) as conn:
            cursor = conn.cursor()
            
            # æœ€è¿‘1æ™‚é–“ã®ã‚¨ãƒ©ãƒ¼çµ±è¨ˆ
            cursor.execute("""
                SELECT COUNT(*) as total_errors,
                       COUNT(DISTINCT error_type) as unique_error_types
                FROM http_error_analytics 
                WHERE session_id = ? AND timestamp > ?
            """, (self.session_id, current_time - 3600))
            
            recent_stats = cursor.fetchone()
            
            # æœ€æ–°ã®æ™‚é–“åˆ¥çµ±è¨ˆ
            cursor.execute("""
                SELECT hour_offset, success_rate, total_errors
                FROM hourly_error_stats 
                WHERE session_id = ?
                ORDER BY hour_offset DESC
                LIMIT 1
            """, (self.session_id,))
            
            latest_hourly = cursor.fetchone()
        
        return {
            "session_id": self.session_id,
            "recent_errors_1h": recent_stats[0] if recent_stats else 0,
            "unique_error_types_1h": recent_stats[1] if recent_stats else 0,
            "current_hour_success_rate": latest_hourly[1] if latest_hourly else 1.0,
            "current_hour_errors": latest_hourly[2] if latest_hourly else 0,
            "analysis_available": latest_hourly is not None
        }