"""
å‡¦ç†é€Ÿåº¦ä½ä¸‹è‡ªå‹•æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
é•·æœŸç¨¼åƒæ™‚ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ã®æ—©æœŸç™ºè¦‹ã¨å¯¾ç­–ææ¡ˆ
"""

import json
import sqlite3
import time
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple

from .database import DatabaseManager


class PerformanceMonitor:
    """å‡¦ç†é€Ÿåº¦ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, database_manager: DatabaseManager):
        self.db = database_manager
        self.session_id = self._generate_session_id()
        self.baseline_performance = None
        self.performance_history = []
        self.init_performance_tables()
    
    def _generate_session_id(self) -> str:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’ç”Ÿæˆ"""
        return f"perf_{int(time.time())}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    def init_performance_tables(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®åˆæœŸåŒ–"""
        with sqlite3.connect(self.db.db_file) as conn:
            cursor = conn.cursor()
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS performance_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp REAL NOT NULL,
                    session_id TEXT NOT NULL,
                    metric_type TEXT NOT NULL,
                    value REAL NOT NULL,
                    unit TEXT,
                    context_json TEXT,
                    runtime_hours REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # å‡¦ç†é€Ÿåº¦çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS processing_speed_stats (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id TEXT NOT NULL,
                    time_window_start REAL NOT NULL,
                    time_window_end REAL NOT NULL,
                    total_processed INTEGER DEFAULT 0,
                    total_blocked INTEGER DEFAULT 0,
                    total_errors INTEGER DEFAULT 0,
                    avg_processing_time REAL,
                    requests_per_second REAL,
                    success_rate REAL,
                    bottleneck_detected BOOLEAN DEFAULT FALSE,
                    bottleneck_type TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ã‚¢ãƒ©ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS performance_alerts (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id TEXT NOT NULL,
                    alert_type TEXT NOT NULL,
                    severity TEXT NOT NULL,  -- LOW, MEDIUM, HIGH, CRITICAL
                    title TEXT NOT NULL,
                    description TEXT,
                    metrics_json TEXT,
                    recommendations_json TEXT,
                    resolved BOOLEAN DEFAULT FALSE,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            conn.commit()
            print(f"ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ†ãƒ¼ãƒ–ãƒ«åˆæœŸåŒ–å®Œäº†: {self.db.db_file}")
    
    def record_processing_metrics(self, metrics: Dict[str, Any]) -> None:
        """å‡¦ç†ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨˜éŒ²"""
        current_time = time.time()
        runtime_hours = metrics.get('runtime_hours', 0)
        
        with sqlite3.connect(self.db.db_file) as conn:
            cursor = conn.cursor()
            
            # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å€‹åˆ¥ã«è¨˜éŒ²
            for metric_type, value in metrics.items():
                if metric_type in ['runtime_hours', 'context']:
                    continue
                
                cursor.execute("""
                    INSERT INTO performance_metrics 
                    (timestamp, session_id, metric_type, value, unit, context_json, runtime_hours)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    current_time,
                    self.session_id,
                    metric_type,
                    float(value),
                    self._get_metric_unit(metric_type),
                    json.dumps(metrics.get('context', {})),
                    runtime_hours
                ))
    
    def _get_metric_unit(self, metric_type: str) -> str:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¿ã‚¤ãƒ—ã«å¯¾å¿œã™ã‚‹å˜ä½ã‚’å–å¾—"""
        units = {
            'processing_time': 'seconds',
            'requests_per_second': 'rps',
            'success_rate': 'percentage',
            'memory_usage': 'MB',
            'cpu_usage': 'percentage',
            'cache_hit_rate': 'percentage',
            'batch_size': 'count',
            'retry_rate': 'percentage',
            'response_time': 'ms'
        }
        return units.get(metric_type, 'unknown')
    
    def update_processing_window(self, window_data: Dict[str, Any]) -> None:
        """å‡¦ç†ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦çµ±è¨ˆã®æ›´æ–°"""
        with sqlite3.connect(self.db.db_file) as conn:
            cursor = conn.cursor()
            
            # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ¤œå‡º
            bottleneck_detected, bottleneck_type = self._detect_bottleneck(window_data)
            
            cursor.execute("""
                INSERT INTO processing_speed_stats 
                (session_id, time_window_start, time_window_end, total_processed, 
                 total_blocked, total_errors, avg_processing_time, requests_per_second,
                 success_rate, bottleneck_detected, bottleneck_type)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                self.session_id,
                window_data['window_start'],
                window_data['window_end'],
                window_data.get('total_processed', 0),
                window_data.get('total_blocked', 0),
                window_data.get('total_errors', 0),
                window_data.get('avg_processing_time', 0),
                window_data.get('requests_per_second', 0),
                window_data.get('success_rate', 1.0),
                bottleneck_detected,
                bottleneck_type
            ))
    
    def _detect_bottleneck(self, window_data: Dict[str, Any]) -> Tuple[bool, Optional[str]]:
        """ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®æ¤œå‡º"""
        bottlenecks = []
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†é€Ÿåº¦ã®ä½ä¸‹
        rps = window_data.get('requests_per_second', 0)
        if rps < 1.0:  # 1ç§’ã«1ãƒªã‚¯ã‚¨ã‚¹ãƒˆæœªæº€
            bottlenecks.append('low_request_rate')
        
        # å¹³å‡å‡¦ç†æ™‚é–“ã®å¢—åŠ 
        avg_time = window_data.get('avg_processing_time', 0)
        if avg_time > 5.0:  # 5ç§’ä»¥ä¸Š
            bottlenecks.append('high_processing_time')
        
        # æˆåŠŸç‡ã®ä½ä¸‹
        success_rate = window_data.get('success_rate', 1.0)
        if success_rate < 0.8:  # 80%æœªæº€
            bottlenecks.append('low_success_rate')
        
        # ã‚¨ãƒ©ãƒ¼ç‡ã®å¢—åŠ 
        total_requests = window_data.get('total_processed', 0) + window_data.get('total_errors', 0)
        if total_requests > 0:
            error_rate = window_data.get('total_errors', 0) / total_requests
            if error_rate > 0.2:  # 20%ä»¥ä¸Š
                bottlenecks.append('high_error_rate')
        
        return len(bottlenecks) > 0, ','.join(bottlenecks) if bottlenecks else None
    
    def analyze_performance_degradation(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ã®åˆ†æ"""
        current_time = time.time()
        
        with sqlite3.connect(self.db.db_file) as conn:
            cursor = conn.cursor()
            
            # æœ€è¿‘1æ™‚é–“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
            cursor.execute("""
                SELECT metric_type, AVG(value) as avg_value, COUNT(*) as count
                FROM performance_metrics 
                WHERE session_id = ? AND timestamp > ?
                GROUP BY metric_type
            """, (self.session_id, current_time - 3600))
            
            recent_metrics = {row[0]: {'avg': row[1], 'count': row[2]} for row in cursor.fetchall()}
            
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒç”¨ã®åˆæœŸ1æ™‚é–“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            cursor.execute("""
                SELECT metric_type, AVG(value) as avg_value, COUNT(*) as count
                FROM performance_metrics 
                WHERE session_id = ? AND runtime_hours <= 1.0
                GROUP BY metric_type
            """, (self.session_id,))
            
            baseline_metrics = {row[0]: {'avg': row[1], 'count': row[2]} for row in cursor.fetchall()}
            
            # å‡¦ç†é€Ÿåº¦çµ±è¨ˆã®å–å¾—
            cursor.execute("""
                SELECT AVG(requests_per_second) as avg_rps,
                       AVG(avg_processing_time) as avg_proc_time,
                       AVG(success_rate) as avg_success_rate,
                       COUNT(CASE WHEN bottleneck_detected THEN 1 END) as bottleneck_count,
                       COUNT(*) as total_windows
                FROM processing_speed_stats 
                WHERE session_id = ?
            """, (self.session_id,))
            
            speed_stats = cursor.fetchone()
        
        # åŠ£åŒ–åˆ†æ
        degradation_analysis = {
            "status": "analysis_complete",
            "baseline_comparison": self._compare_with_baseline(recent_metrics, baseline_metrics),
            "processing_speed_trends": {
                "avg_rps": speed_stats[0] if speed_stats else 0,
                "avg_processing_time": speed_stats[1] if speed_stats else 0,
                "avg_success_rate": speed_stats[2] if speed_stats else 1.0,
                "bottleneck_frequency": speed_stats[3] / max(speed_stats[4], 1) if speed_stats and speed_stats[4] > 0 else 0
            },
            "degradation_patterns": self._identify_degradation_patterns(recent_metrics, baseline_metrics),
            "recommendations": []
        }
        
        # æ¨å¥¨äº‹é …ã®ç”Ÿæˆ
        degradation_analysis["recommendations"] = self._generate_performance_recommendations(degradation_analysis)
        
        return degradation_analysis
    
    def _compare_with_baseline(self, recent: Dict, baseline: Dict) -> Dict[str, Any]:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒåˆ†æ"""
        comparison = {}
        
        for metric_type in set(recent.keys()) | set(baseline.keys()):
            recent_val = recent.get(metric_type, {}).get('avg', 0)
            baseline_val = baseline.get(metric_type, {}).get('avg', 0)
            
            if baseline_val > 0:
                change_percent = ((recent_val - baseline_val) / baseline_val) * 100
                comparison[metric_type] = {
                    "recent_avg": recent_val,
                    "baseline_avg": baseline_val,
                    "change_percent": change_percent,
                    "trend": "improved" if change_percent > 5 else "degraded" if change_percent < -5 else "stable"
                }
        
        return comparison
    
    def _identify_degradation_patterns(self, recent: Dict, baseline: Dict) -> Dict[str, bool]:
        """åŠ£åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å®š"""
        patterns = {
            "processing_time_increase": False,
            "request_rate_decrease": False,
            "success_rate_decline": False,
            "memory_leak_suspected": False,
            "cache_efficiency_drop": False
        }
        
        comparison = self._compare_with_baseline(recent, baseline)
        
        # å‡¦ç†æ™‚é–“ã®å¢—åŠ 
        if "processing_time" in comparison:
            if comparison["processing_time"]["change_percent"] > 50:  # 50%ä»¥ä¸Šå¢—åŠ 
                patterns["processing_time_increase"] = True
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆç‡ã®ä½ä¸‹
        if "requests_per_second" in comparison:
            if comparison["requests_per_second"]["change_percent"] < -30:  # 30%ä»¥ä¸Šä½ä¸‹
                patterns["request_rate_decrease"] = True
        
        # æˆåŠŸç‡ã®ä½ä¸‹
        if "success_rate" in comparison:
            if comparison["success_rate"]["change_percent"] < -10:  # 10%ä»¥ä¸Šä½ä¸‹
                patterns["success_rate_decline"] = True
        
        # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ç–‘ã„
        if "memory_usage" in comparison:
            if comparison["memory_usage"]["change_percent"] > 100:  # 100%ä»¥ä¸Šå¢—åŠ 
                patterns["memory_leak_suspected"] = True
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ä½ä¸‹
        if "cache_hit_rate" in comparison:
            if comparison["cache_hit_rate"]["change_percent"] < -20:  # 20%ä»¥ä¸Šä½ä¸‹
                patterns["cache_efficiency_drop"] = True
        
        return patterns
    
    def _generate_performance_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []
        patterns = analysis.get("degradation_patterns", {})
        speed_trends = analysis.get("processing_speed_trends", {})
        
        # å‡¦ç†æ™‚é–“å¢—åŠ ã¸ã®å¯¾å¿œ
        if patterns.get("processing_time_increase"):
            recommendations.append("ğŸŒ å‡¦ç†æ™‚é–“ã®å¤§å¹…å¢—åŠ æ¤œå‡º - ãƒãƒƒãƒã‚µã‚¤ã‚ºã®èª¿æ•´ã‚’æ¨å¥¨")
            recommendations.append("ğŸ”§ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«ã®æœ€é©åŒ–ã‚’æ¤œè¨")
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆç‡ä½ä¸‹ã¸ã®å¯¾å¿œ
        if patterns.get("request_rate_decrease"):
            recommendations.append("ğŸ“‰ ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ç‡ä½ä¸‹ - ä¸¦è¡Œå‡¦ç†æ•°ã®è¦‹ç›´ã—ãŒå¿…è¦")
            recommendations.append("âš¡ ãƒ¬ãƒ¼ãƒˆåˆ¶é™è¨­å®šã®ç·©å’Œã‚’æ¤œè¨")
        
        # æˆåŠŸç‡ä½ä¸‹ã¸ã®å¯¾å¿œ
        if patterns.get("success_rate_decline"):
            recommendations.append("âŒ æˆåŠŸç‡ä½ä¸‹æ¤œå‡º - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®å¼·åŒ–ãŒå¿…è¦")
            recommendations.append("ğŸ”„ ãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥ã®è¦‹ç›´ã—ã‚’æ¨å¥¨")
        
        # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ç–‘ã„ã¸ã®å¯¾å¿œ
        if patterns.get("memory_leak_suspected"):
            recommendations.append("ğŸ§  ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ç–‘ã„ - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã®å®Ÿè¡Œã‚’æ¨å¥¨")
            recommendations.append("ğŸ”„ å®šæœŸçš„ãªã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒªã‚»ãƒƒãƒˆã®å®Ÿè£…ã‚’æ¤œè¨")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ä½ä¸‹ã¸ã®å¯¾å¿œ
        if patterns.get("cache_efficiency_drop"):
            recommendations.append("ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ä½ä¸‹ - ã‚­ãƒ£ãƒƒã‚·ãƒ¥TTLã®èª¿æ•´ãŒå¿…è¦")
            recommendations.append("ğŸ—‚ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã®è¦‹ç›´ã—ã‚’æ¨å¥¨")
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯é »åº¦ã«ã‚ˆã‚‹æ¨å¥¨
        bottleneck_freq = speed_trends.get("bottleneck_frequency", 0)
        if bottleneck_freq > 0.3:  # 30%ä»¥ä¸Šã§ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
            recommendations.append(f"ğŸš§ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯é »ç™º ({bottleneck_freq:.1%}) - ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®è¦‹ç›´ã—ãŒå¿…è¦")
        
        # å…¨ä½“çš„ãªæ¨å¥¨äº‹é …
        avg_rps = speed_trends.get("avg_rps", 0)
        if avg_rps < 0.5:  # 0.5rpsæœªæº€
            recommendations.append("ğŸ¢ æ¥µç«¯ãªå‡¦ç†é€Ÿåº¦ä½ä¸‹ - ç·Šæ€¥å¯¾å¿œãŒå¿…è¦")
            recommendations.append("ğŸ› ï¸ ã‚·ã‚¹ãƒ†ãƒ å†èµ·å‹•ã®æ¤œè¨ã‚’æ¨å¥¨")
        
        return recommendations
    
    def create_performance_alert(self, alert_type: str, severity: str, 
                                title: str, description: str = "", 
                                metrics: Dict = None, recommendations: List[str] = None) -> None:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¢ãƒ©ãƒ¼ãƒˆã®ç”Ÿæˆ"""
        with sqlite3.connect(self.db.db_file) as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT INTO performance_alerts 
                (session_id, alert_type, severity, title, description, 
                 metrics_json, recommendations_json)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                self.session_id,
                alert_type,
                severity,
                title,
                description,
                json.dumps(metrics or {}),
                json.dumps(recommendations or [])
            ))
    
    def check_degradation_thresholds(self, current_metrics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åŠ£åŒ–é–¾å€¤ã®ãƒã‚§ãƒƒã‚¯ã¨ã‚¢ãƒ©ãƒ¼ãƒˆç”Ÿæˆ"""
        alerts = []
        
        # å‡¦ç†é€Ÿåº¦ã®é‡å¤§ãªä½ä¸‹
        rps = current_metrics.get('requests_per_second', 0)
        if rps < 0.1:  # 0.1rpsæœªæº€
            alert = {
                "type": "critical_speed_degradation",
                "severity": "CRITICAL",
                "title": "å‡¦ç†é€Ÿåº¦ã®é‡å¤§ãªä½ä¸‹",
                "description": f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†é€Ÿåº¦ãŒ{rps:.3f}rpsã¾ã§ä½ä¸‹",
                "recommendations": [
                    "å³åº§ã«ã‚·ã‚¹ãƒ†ãƒ èª¿æŸ»ã‚’å®Ÿè¡Œ",
                    "ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åŸå› ã®ç‰¹å®š",
                    "ç·Šæ€¥ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã®æ¤œè¨"
                ]
            }
            alerts.append(alert)
            self.create_performance_alert(**alert)
        
        # æˆåŠŸç‡ã®é‡å¤§ãªä½ä¸‹
        success_rate = current_metrics.get('success_rate', 1.0)
        if success_rate < 0.5:  # 50%æœªæº€
            alert = {
                "type": "critical_success_rate_drop",
                "severity": "HIGH",
                "title": "æˆåŠŸç‡ã®é‡å¤§ãªä½ä¸‹",
                "description": f"å‡¦ç†æˆåŠŸç‡ãŒ{success_rate:.1%}ã¾ã§ä½ä¸‹",
                "recommendations": [
                    "ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®è©³ç´°ç¢ºèª",
                    "èªè¨¼çŠ¶æ…‹ã®å†ç¢ºèª",
                    "Cookieå†èª­ã¿è¾¼ã¿ã®å®Ÿè¡Œ"
                ]
            }
            alerts.append(alert)
            self.create_performance_alert(**alert)
        
        # å¹³å‡å‡¦ç†æ™‚é–“ã®ç•°å¸¸å¢—åŠ 
        avg_time = current_metrics.get('avg_processing_time', 0)
        if avg_time > 10.0:  # 10ç§’ä»¥ä¸Š
            alert = {
                "type": "high_processing_time",
                "severity": "MEDIUM",
                "title": "å‡¦ç†æ™‚é–“ã®ç•°å¸¸å¢—åŠ ",
                "description": f"å¹³å‡å‡¦ç†æ™‚é–“ãŒ{avg_time:.1f}ç§’ã¾ã§å¢—åŠ ",
                "recommendations": [
                    "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šçŠ¶æ…‹ã®ç¢ºèª",
                    "ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ã®ç¢ºèª",
                    "ãƒãƒƒãƒã‚µã‚¤ã‚ºã®èª¿æ•´"
                ]
            }
            alerts.append(alert)
            self.create_performance_alert(**alert)
        
        return alerts
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦ã®å–å¾—"""
        current_time = time.time()
        
        with sqlite3.connect(self.db.db_file) as conn:
            cursor = conn.cursor()
            
            # æœ€æ–°ã®çµ±è¨ˆ
            cursor.execute("""
                SELECT requests_per_second, avg_processing_time, success_rate,
                       bottleneck_detected, bottleneck_type
                FROM processing_speed_stats 
                WHERE session_id = ?
                ORDER BY time_window_end DESC
                LIMIT 1
            """, (self.session_id,))
            
            latest_stats = cursor.fetchone()
            
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¢ãƒ©ãƒ¼ãƒˆæ•°
            cursor.execute("""
                SELECT COUNT(*) as active_alerts,
                       COUNT(CASE WHEN severity = 'CRITICAL' THEN 1 END) as critical_alerts
                FROM performance_alerts 
                WHERE session_id = ? AND resolved = FALSE
            """, (self.session_id,))
            
            alert_stats = cursor.fetchone()
        
        summary = {
            "session_id": self.session_id,
            "monitoring_active": True,
            "latest_performance": {
                "requests_per_second": latest_stats[0] if latest_stats else 0,
                "avg_processing_time": latest_stats[1] if latest_stats else 0,
                "success_rate": latest_stats[2] if latest_stats else 1.0,
                "bottleneck_detected": latest_stats[3] if latest_stats else False,
                "bottleneck_type": latest_stats[4] if latest_stats else None
            },
            "alerts": {
                "active_count": alert_stats[0] if alert_stats else 0,
                "critical_count": alert_stats[1] if alert_stats else 0
            },
            "recommendations_available": True
        }
        
        return summary