# æ¨å¥¨å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ‘ã‚¿ãƒ¼ãƒ³

### è²¬å‹™åˆ†é›¢ãƒ‘ã‚¿ãƒ¼ãƒ³
```python
# âœ… æ¨å¥¨: å„ã‚¯ãƒ©ã‚¹ã®è²¬å‹™ã‚’æ˜ç¢ºã«åˆ†é›¢

class TwitterAPI:
    """Twitter APIé€šä¿¡å°‚é–€"""
    def get_user_info(self, screen_name): pass
    def block_user(self, user_id): pass

class DatabaseManager:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œå°‚é–€"""
    def save_block_history(self, user_id, status): pass
    def get_retry_targets(self): pass

class CacheManager:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†å°‚é–€"""
    def get_from_cache(self, key): pass
    def save_to_cache(self, key, data): pass

class BlockManager:
    """ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯çµ±æ‹¬"""
    def __init__(self):
        self.api = TwitterAPI()
        self.database = DatabaseManager()
        self.cache = CacheManager()
    
    def process_block_request(self, screen_name):
        # å„å°‚é–€ã‚¯ãƒ©ã‚¹ã‚’çµ„ã¿åˆã‚ã›ã¦å‡¦ç†
        pass
```

### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³
```python
# âœ… æ¨å¥¨: çµ±ä¸€çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

def api_operation_with_recovery(operation_func, *args, **kwargs):
    """APIæ“ä½œã®çµ±ä¸€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°"""
    max_retries = 3
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            return operation_func(*args, **kwargs)
        except PermanentError as e:
            # æ°¸ç¶šçš„å¤±æ•—â†’å³åº§ã«è«¦ã‚ã‚‹
            raise e
        except TemporaryError as e:
            # ä¸€æ™‚çš„å¤±æ•—â†’ãƒªãƒˆãƒ©ã‚¤
            retry_count += 1
            wait_time = calculate_backoff_time(retry_count)
            time.sleep(wait_time)
        except UnknownError as e:
            # ä¸æ˜ã‚¨ãƒ©ãƒ¼â†’1å›ã ã‘ãƒªãƒˆãƒ©ã‚¤
            if retry_count == 0:
                retry_count = max_retries - 1
                time.sleep(60)
            else:
                raise e
    
    raise MaxRetriesExceededError()

# ä½¿ç”¨ä¾‹
def block_user_safely(self, user_id):
    return api_operation_with_recovery(
        self.api.block_user_by_id, 
        user_id
    )
```

## ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³

### ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³
```python
# âœ… æ¨å¥¨: ãƒãƒƒãƒã‚¯ã‚¨ãƒªã§N+1å•é¡Œã‚’å›é¿

def get_permanent_failures_batch(self, identifiers, user_format="screen_name"):
    """è¤‡æ•°ã®æ°¸ç¶šçš„å¤±æ•—ã‚’ä¸€æ‹¬å–å¾—"""
    if not identifiers:
        return {}
    
    placeholders = ",".join("?" * len(identifiers))
    
    # å‹•çš„ã‚¯ã‚¨ãƒªç”Ÿæˆ
    if user_format == "user_id":
        query = f"""
            SELECT user_id, user_status, error_message, updated_at
            FROM block_history 
            WHERE user_id IN ({placeholders}) 
            AND status = 'failed'
            AND user_status IN ('suspended', 'not_found', 'deactivated')
        """
    else:
        query = f"""
            SELECT screen_name, user_status, error_message, updated_at
            FROM block_history 
            WHERE screen_name IN ({placeholders}) 
            AND status = 'failed' 
            AND user_status IN ('suspended', 'not_found', 'deactivated')
        """
    
    with sqlite3.connect(self.db_file) as conn:
        cursor = conn.cursor()
        cursor.execute(query, [str(id_) for id_ in identifiers])
        
        return {
            row[0]: {
                "user_status": row[1],
                "error_message": row[2],
                "updated_at": row[3]
            }
            for row in cursor.fetchall()
        }

# ä½¿ç”¨ä¾‹
def process_users_efficiently(self, user_list):
    # 1. ãƒãƒƒãƒã§æ°¸ç¶šçš„å¤±æ•—ã‚’ãƒã‚§ãƒƒã‚¯
    permanent_failures = self.database.get_permanent_failures_batch(user_list)
    
    # 2. å‡¦ç†å¯¾è±¡ã‚’çµã‚Šè¾¼ã¿
    processable_users = [u for u in user_list if u not in permanent_failures]
    
    # 3. ãƒãƒƒãƒã§APIå‡¦ç†
    results = self.api.process_users_batch(processable_users)
    
    return results
```

### ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ãƒ‘ã‚¿ãƒ¼ãƒ³
```python
# âœ… æ¨å¥¨: context managerä½¿ç”¨

class DatabaseConnection:
    def __init__(self, db_file):
        self.db_file = db_file
        self.conn = None
    
    def __enter__(self):
        self.conn = sqlite3.connect(self.db_file)
        return self.conn
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.conn:
            if exc_type is None:
                self.conn.commit()
            else:
                self.conn.rollback()
            self.conn.close()

# ä½¿ç”¨ä¾‹
def save_block_result(self, user_id, result):
    with DatabaseConnection(self.db_file) as conn:
        cursor = conn.cursor()
        cursor.execute("""
            INSERT INTO block_history (user_id, status, timestamp)
            VALUES (?, ?, ?)
        """, (user_id, result, time.time()))
        # è‡ªå‹•çš„ã«commit/rollback/closeãŒå®Ÿè¡Œã•ã‚Œã‚‹
```

## ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³

### éšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ã‚¿ãƒ¼ãƒ³
```python
# âœ… æ¨å¥¨: 3å±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®åŠ¹ç‡çš„åˆ©ç”¨

def get_user_with_hierarchical_cache(self, screen_name):
    """éšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ´»ç”¨ã—ãŸæœ€é©ãªå–å¾—"""
    
    # 1å±¤ç›®: screen_name â†’ user_id å¤‰æ›
    user_id = self.cache.get_lookup(screen_name)
    if not user_id:
        # APIã§lookupå®Ÿè¡Œ
        user_id = self.api.get_user_id_by_screen_name(screen_name)
        self.cache.save_lookup(screen_name, user_id)
    
    # 2å±¤ç›®: ãƒ¦ãƒ¼ã‚¶ãƒ¼åŸºæœ¬æƒ…å ±
    profile = self.cache.get_profile(user_id)
    
    # 3å±¤ç›®: ãƒ•ã‚©ãƒ­ãƒ¼ãƒ»ãƒ–ãƒ­ãƒƒã‚¯é–¢ä¿‚
    relationships = self.cache.get_relationships(user_id)
    
    missing_data = []
    if not profile:
        missing_data.append("profile")
    if not relationships:
        missing_data.append("relationships")
    
    if missing_data:
        # å¿…è¦æœ€å°é™ã®APIå‘¼ã³å‡ºã—
        api_data = self.api.get_user_data(user_id, include=missing_data)
        
        if "profile" in missing_data:
            profile = api_data["profile"]
            self.cache.save_profile(user_id, profile)
        
        if "relationships" in missing_data:
            relationships = api_data["relationships"]
            self.cache.save_relationships(user_id, relationships)
    
    return self.combine_user_data(profile, relationships)
```

### ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³
```python
# âœ… æ¨å¥¨: é©åˆ‡ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–

def invalidate_user_cache(self, user_id, screen_name=None):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼é–¢é€£ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç„¡åŠ¹åŒ–"""
    
    # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒ»é–¢ä¿‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å¸¸ã«ç„¡åŠ¹åŒ–
    self.cache.invalidate_profile(user_id)
    self.cache.invalidate_relationships(user_id)
    
    # lookup ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯é¸æŠçš„ã«ç„¡åŠ¹åŒ–
    if screen_name:
        self.cache.invalidate_lookup(screen_name)
    
    # æ“ä½œãƒ­ã‚°
    print(f"ğŸ—‘ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–: user_id={user_id}")

def block_user_with_cache_update(self, user_id, screen_name):
    """ãƒ–ãƒ­ãƒƒã‚¯æ“ä½œã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°"""
    
    # ãƒ–ãƒ­ãƒƒã‚¯å®Ÿè¡Œ
    result = self.api.block_user_by_id(user_id)
    
    if result["success"]:
        # é–¢ä¿‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã¿ç„¡åŠ¹åŒ–ï¼ˆãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã¯æ®‹ã™ï¼‰
        self.cache.invalidate_relationships(user_id)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨˜éŒ²
        self.database.save_block_history(user_id, "success")
    
    return result
```

## éåŒæœŸãƒ»ä¸¦è¡Œå‡¦ç†ãƒ‘ã‚¿ãƒ¼ãƒ³

### ãƒãƒƒãƒå‡¦ç†ã®ä¸¦è¡Œå®Ÿè¡Œ
```python
# âœ… æ¨å¥¨: ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ã—ãŸä¸¦è¡Œå‡¦ç†

import asyncio
import aiohttp
from asyncio import Semaphore

class ConcurrentProcessor:
    def __init__(self, max_concurrent=5):
        self.semaphore = Semaphore(max_concurrent)
        self.rate_limiter = RateLimiter(requests_per_window=150, window_seconds=900)
    
    async def process_user_batch(self, user_list):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¹ãƒˆã®ä¸¦è¡Œå‡¦ç†"""
        tasks = []
        
        for user in user_list:
            task = self.process_single_user_with_limits(user)
            tasks.append(task)
        
        return await asyncio.gather(*tasks, return_exceptions=True)
    
    async def process_single_user_with_limits(self, user):
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ»ä¸¦è¡Œåˆ¶é™ä»˜ãå˜ä¸€ãƒ¦ãƒ¼ã‚¶ãƒ¼å‡¦ç†"""
        async with self.semaphore:  # ä¸¦è¡Œæ•°åˆ¶é™
            await self.rate_limiter.wait()  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            return await self.process_user(user)
    
    async def process_user(self, user):
        """å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å‡¦ç†"""
        # æ°¸ç¶šçš„å¤±æ•—ãƒã‚§ãƒƒã‚¯
        if self.database.is_permanent_failure(user):
            return {"user": user, "status": "skipped", "reason": "permanent_failure"}
        
        # APIå‡¦ç†
        try:
            result = await self.api.block_user_async(user)
            return {"user": user, "status": "success", "result": result}
        except Exception as e:
            return {"user": user, "status": "error", "error": str(e)}
```

### ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ãƒ‘ã‚¿ãƒ¼ãƒ³
```python
# âœ… æ¨å¥¨: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†

def process_large_user_list_streaming(self, user_file_path, batch_size=50):
    """å¤§é‡ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¹ãƒˆã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†"""
    
    def user_generator():
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ãƒãƒƒãƒå˜ä½ã§é †æ¬¡èª­ã¿è¾¼ã¿"""
        with open(user_file_path, 'r') as f:
            users = json.load(f)
            
            for i in range(0, len(users), batch_size):
                yield users[i:i + batch_size]
    
    total_processed = 0
    total_success = 0
    
    for batch in user_generator():
        # ãƒãƒƒãƒå‡¦ç†
        batch_results = self.process_user_batch(batch)
        
        # çµ±è¨ˆæ›´æ–°
        total_processed += len(batch)
        total_success += sum(1 for r in batch_results if r.get("status") == "success")
        
        # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        del batch_results
        
        # é€²æ—è¡¨ç¤º
        print(f"ğŸ“Š å‡¦ç†é€²æ—: {total_processed}ä»¶å®Œäº† (æˆåŠŸ: {total_success}ä»¶)")
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã®ä¼‘æ†©
        time.sleep(1)
    
    return {"total_processed": total_processed, "total_success": total_success}
```

## è¨­å®šç®¡ç†ãƒ‘ã‚¿ãƒ¼ãƒ³

### ç’°å¢ƒåˆ¥è¨­å®šãƒ‘ã‚¿ãƒ¼ãƒ³
```python
# âœ… æ¨å¥¨: ç’°å¢ƒã«å¿œã˜ãŸå‹•çš„è¨­å®š

import os

class Config:
    def __init__(self):
        self.environment = os.getenv('ENVIRONMENT', 'development')
        self.load_config()
    
    def load_config(self):
        """ç’°å¢ƒåˆ¥è¨­å®šã®èª­ã¿è¾¼ã¿"""
        base_config = {
            'batch_size': 50,
            'retry_count': 3,
            'cache_ttl': 3600,
            'api_delay': 1.0
        }
        
        env_configs = {
            'development': {
                'batch_size': 10,  # å°ã•ã„ãƒãƒƒãƒã§å®‰å…¨ã«
                'api_delay': 2.0,  # ã‚†ã£ãã‚Šå®Ÿè¡Œ
                'debug_mode': True
            },
            'production': {
                'batch_size': 100,  # é«˜åŠ¹ç‡ãƒãƒƒãƒ
                'api_delay': 0.5,   # é«˜é€Ÿå®Ÿè¡Œ
                'debug_mode': False
            },
            'test': {
                'batch_size': 5,
                'api_delay': 0.1,
                'mock_api': True
            }
        }
        
        # åŸºæœ¬è¨­å®šã‚’ãƒãƒ¼ã‚¸
        self.config = base_config.copy()
        self.config.update(env_configs.get(self.environment, {}))
        
        # ç’°å¢ƒå¤‰æ•°ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
        self.apply_env_overrides()
    
    def apply_env_overrides(self):
        """ç’°å¢ƒå¤‰æ•°ã«ã‚ˆã‚‹è¨­å®šã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰"""
        overrides = {
            'BATCH_SIZE': ('batch_size', int),
            'API_DELAY': ('api_delay', float),
            'RETRY_COUNT': ('retry_count', int)
        }
        
        for env_var, (config_key, type_func) in overrides.items():
            if env_value := os.getenv(env_var):
                self.config[config_key] = type_func(env_value)
    
    def get(self, key, default=None):
        """è¨­å®šå€¤ã®å–å¾—"""
        return self.config.get(key, default)

# ä½¿ç”¨ä¾‹
config = Config()
batch_size = config.get('batch_size')
api_delay = config.get('api_delay')
```

## ãƒ­ã‚®ãƒ³ã‚°ãƒ»ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³

### æ§‹é€ åŒ–ãƒ­ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³
```python
# âœ… æ¨å¥¨: æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ­ã‚°å‡ºåŠ›

import json
import datetime

class StructuredLogger:
    def __init__(self, component_name):
        self.component = component_name
    
    def log(self, level, message, **kwargs):
        """æ§‹é€ åŒ–ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒª"""
        log_entry = {
            'timestamp': datetime.datetime.now().isoformat(),
            'level': level,
            'component': self.component,
            'message': message,
            'data': kwargs
        }
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›ï¼ˆäººé–“å¯èª­ï¼‰
        print(f"[{level}] {self.component}: {message}")
        if kwargs:
            for key, value in kwargs.items():
                print(f"  {key}: {value}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ï¼ˆJSONï¼‰
        with open('application.log', 'a') as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
    
    def info(self, message, **kwargs):
        self.log('INFO', message, **kwargs)
    
    def warning(self, message, **kwargs):
        self.log('WARNING', message, **kwargs)
    
    def error(self, message, **kwargs):
        self.log('ERROR', message, **kwargs)

# ä½¿ç”¨ä¾‹
logger = StructuredLogger('BlockManager')

def process_block_request(self, user_id):
    logger.info("ãƒ–ãƒ­ãƒƒã‚¯å‡¦ç†é–‹å§‹", user_id=user_id)
    
    try:
        result = self.api.block_user_by_id(user_id)
        logger.info("ãƒ–ãƒ­ãƒƒã‚¯å®Œäº†", user_id=user_id, success=True)
        return result
    except Exception as e:
        logger.error("ãƒ–ãƒ­ãƒƒã‚¯å¤±æ•—", user_id=user_id, error=str(e))
        raise
```