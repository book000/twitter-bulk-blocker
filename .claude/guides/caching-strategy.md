# 3å±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥è©³ç´°

## ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

### 3å±¤æ§‹é€ 
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Lookup Cache â”‚ â† screen_name â†’ user_id å¤‰æ›
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. Profile Cacheâ”‚ â† ãƒ¦ãƒ¼ã‚¶ãƒ¼åŸºæœ¬æƒ…å ±
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  
â”‚ 3. Relation Cacheâ”‚ â† ãƒ•ã‚©ãƒ­ãƒ¼ãƒ»ãƒ–ãƒ­ãƒƒã‚¯é–¢ä¿‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å„å±¤ã®å½¹å‰²ã¨ç‰¹å¾´
```python
CACHE_LAYERS = {
    "lookup": {
        "purpose": "screen_name â†’ user_id å¤‰æ›",
        "ttl": 86400,  # 24æ™‚é–“
        "size_limit": "unlimited",
        "key_format": "screen_name",
        "value_format": "user_id"
    },
    "profiles": {
        "purpose": "ãƒ¦ãƒ¼ã‚¶ãƒ¼åŸºæœ¬æƒ…å ±",
        "ttl": 3600,   # 1æ™‚é–“
        "size_limit": "1000 files",
        "key_format": "user_id", 
        "value_format": "full_user_object"
    },
    "relationships": {
        "purpose": "ãƒ•ã‚©ãƒ­ãƒ¼ãƒ»ãƒ–ãƒ­ãƒƒã‚¯é–¢ä¿‚",
        "ttl": 1800,   # 30åˆ†
        "size_limit": "500 files",
        "key_format": "user_id",
        "value_format": "relationship_object"
    }
}
```

## å„å±¤ã®å®Ÿè£…è©³ç´°

### 1. Lookup Cacheï¼ˆæœ€é•·æœŸé–“ï¼‰
```python
def get_lookup_from_cache(self, screen_name):
    """screen_name â†’ user_id å¤‰æ›ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    cache_file = self.lookups_cache_dir / f"{screen_name}.json"
    
    if cache_file.exists():
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # TTLãƒã‚§ãƒƒã‚¯ï¼ˆ24æ™‚é–“ï¼‰
            if time.time() - cache_data['timestamp'] < 86400:
                return cache_data['user_id']
            else:
                cache_file.unlink()  # æœŸé™åˆ‡ã‚Œå‰Šé™¤
        except (json.JSONDecodeError, KeyError):
            cache_file.unlink()  # ç ´æãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
    
    return None

def save_lookup_to_cache(self, screen_name, user_id):
    """lookupçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
    cache_data = {
        'user_id': user_id,
        'timestamp': time.time(),
        'screen_name': screen_name
    }
    
    cache_file = self.lookups_cache_dir / f"{screen_name}.json"
    with open(cache_file, 'w', encoding='utf-8') as f:
        json.dump(cache_data, f, ensure_ascii=False, indent=2)
```

### 2. Profile Cacheï¼ˆä¸­æœŸé–“ï¼‰
```python
def get_profile_from_cache(self, user_id):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼åŸºæœ¬æƒ…å ±ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    cache_file = self.profiles_cache_dir / f"{user_id}.json"
    
    if cache_file.exists():
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # TTLãƒã‚§ãƒƒã‚¯ï¼ˆ1æ™‚é–“ï¼‰
            if time.time() - cache_data['timestamp'] < 3600:
                return cache_data['user_data']
            else:
                cache_file.unlink()
        except (json.JSONDecodeError, KeyError):
            cache_file.unlink()
    
    return None

def save_profile_to_cache(self, user_id, user_data):
    """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
    cache_data = {
        'user_data': user_data,
        'timestamp': time.time(),
        'user_id': user_id
    }
    
    cache_file = self.profiles_cache_dir / f"{user_id}.json"
    with open(cache_file, 'w', encoding='utf-8') as f:
        json.dump(cache_data, f, ensure_ascii=False, indent=2)
```

### 3. Relationship Cacheï¼ˆçŸ­æœŸé–“ï¼‰
```python
def get_relationship_from_cache(self, user_id):
    """ãƒ•ã‚©ãƒ­ãƒ¼ãƒ»ãƒ–ãƒ­ãƒƒã‚¯é–¢ä¿‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    cache_file = self.relationships_cache_dir / f"{user_id}.json"
    
    if cache_file.exists():
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # TTLãƒã‚§ãƒƒã‚¯ï¼ˆ30åˆ†ï¼‰
            if time.time() - cache_data['timestamp'] < 1800:
                return cache_data['relationship_data']
            else:
                cache_file.unlink()
        except (json.JSONDecodeError, KeyError):
            cache_file.unlink()
    
    return None

def save_relationship_to_cache(self, user_id, relationship_data):
    """é–¢ä¿‚æƒ…å ±ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
    cache_data = {
        'relationship_data': relationship_data,
        'timestamp': time.time(),
        'user_id': user_id
    }
    
    cache_file = self.relationships_cache_dir / f"{user_id}.json"
    with open(cache_file, 'w', encoding='utf-8') as f:
        json.dump(cache_data, f, ensure_ascii=False, indent=2)
```

## éšå±¤çš„ã‚¢ã‚¯ã‚»ã‚¹æˆ¦ç•¥

### æ®µéšçš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
```python
def get_user_info_with_cache_hierarchy(self, screen_name):
    """3å±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ´»ç”¨ã—ãŸåŠ¹ç‡çš„ãªãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—"""
    
    # Level 1: Lookup Cacheç¢ºèª
    user_id = self.get_lookup_from_cache(screen_name)
    
    if user_id:
        # Level 2: Profile Cacheç¢ºèª
        profile_data = self.get_profile_from_cache(user_id)
        
        # Level 3: Relationship Cacheç¢ºèª
        relationship_data = self.get_relationship_from_cache(user_id)
        
        # å®Œå…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ
        if profile_data and relationship_data:
            return self.combine_user_data(profile_data, relationship_data)
        
        # éƒ¨åˆ†ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆâ†’æœ€å°APIå‘¼ã³å‡ºã—
        elif profile_data:
            # é–¢ä¿‚æƒ…å ±ã®ã¿APIå–å¾—
            relationship_data = self.fetch_relationship_only(user_id)
            self.save_relationship_to_cache(user_id, relationship_data)
            return self.combine_user_data(profile_data, relationship_data)
        
        elif relationship_data:
            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ã®ã¿APIå–å¾—
            profile_data = self.fetch_profile_only(user_id)
            self.save_profile_to_cache(user_id, profile_data)
            return self.combine_user_data(profile_data, relationship_data)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹â†’ãƒ•ãƒ«APIå–å¾—
    return self.fetch_user_info_full(screen_name)
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
```python
def get_users_batch_with_cache(self, screen_names):
    """ãƒãƒƒãƒå‡¦ç†ã§ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨"""
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ³ã®äº‹å‰åˆ†æ
    cache_analysis = self.analyze_cache_coverage(screen_names)
    
    # å®Œå…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ
    cached_users = {}
    for screen_name in cache_analysis['full_cache_hit']:
        cached_users[screen_name] = self.get_user_info_with_cache_hierarchy(screen_name)
    
    # éƒ¨åˆ†ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆâ†’æœ€å°API
    partial_cache_users = {}
    for screen_name in cache_analysis['partial_cache_hit']:
        partial_cache_users[screen_name] = self.get_user_info_with_cache_hierarchy(screen_name)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹â†’ãƒãƒƒãƒAPI
    missing_users = {}
    if cache_analysis['cache_miss']:
        missing_users = self.fetch_users_batch_api(cache_analysis['cache_miss'])
        
        # å–å¾—ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        for screen_name, user_data in missing_users.items():
            self.save_all_cache_layers(screen_name, user_data)
    
    # çµæœã‚’ãƒãƒ¼ã‚¸
    all_users = {}
    all_users.update(cached_users)
    all_users.update(partial_cache_users)
    all_users.update(missing_users)
    
    return all_users

def analyze_cache_coverage(self, screen_names):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ"""
    analysis = {
        'full_cache_hit': [],
        'partial_cache_hit': [],
        'cache_miss': []
    }
    
    for screen_name in screen_names:
        user_id = self.get_lookup_from_cache(screen_name)
        
        if user_id:
            has_profile = self.get_profile_from_cache(user_id) is not None
            has_relationship = self.get_relationship_from_cache(user_id) is not None
            
            if has_profile and has_relationship:
                analysis['full_cache_hit'].append(screen_name)
            elif has_profile or has_relationship:
                analysis['partial_cache_hit'].append(screen_name)
            else:
                analysis['cache_miss'].append(screen_name)
        else:
            analysis['cache_miss'].append(screen_name)
    
    return analysis
```

## ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†

### å®¹é‡ç®¡ç†
```python
def manage_cache_size(self):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºç®¡ç†"""
    
    # å„å±¤ã®å®¹é‡ãƒã‚§ãƒƒã‚¯
    for cache_name, cache_dir in [
        ("lookup", self.lookups_cache_dir),
        ("profiles", self.profiles_cache_dir), 
        ("relationships", self.relationships_cache_dir)
    ]:
        cache_files = list(cache_dir.glob("*.json"))
        
        # ã‚µã‚¤ã‚ºåˆ¶é™ãƒã‚§ãƒƒã‚¯
        if len(cache_files) > CACHE_LAYERS[cache_name].get("size_limit", 1000):
            # å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å‰Šé™¤
            cache_files.sort(key=lambda f: f.stat().st_mtime)
            excess_count = len(cache_files) - CACHE_LAYERS[cache_name]["size_limit"]
            
            for old_file in cache_files[:excess_count]:
                old_file.unlink()
                
            print(f"ğŸ“¦ {cache_name} ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {excess_count}ä»¶ã®å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤")

def cleanup_expired_cache(self):
    """æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å‰Šé™¤"""
    current_time = time.time()
    
    for cache_name, cache_dir in [
        ("lookup", self.lookups_cache_dir),
        ("profiles", self.profiles_cache_dir),
        ("relationships", self.relationships_cache_dir)
    ]:
        ttl = CACHE_LAYERS[cache_name]["ttl"]
        expired_count = 0
        
        for cache_file in cache_dir.glob("*.json"):
            if current_time - cache_file.stat().st_mtime > ttl:
                cache_file.unlink()
                expired_count += 1
        
        if expired_count > 0:
            print(f"ğŸ—‘ï¸ {cache_name} ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {expired_count}ä»¶ã®æœŸé™åˆ‡ã‚Œãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤")
```

### ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ
```python
def get_cache_statistics(self):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆæƒ…å ±"""
    stats = {}
    
    for cache_name, cache_dir in [
        ("lookup", self.lookups_cache_dir),
        ("profiles", self.profiles_cache_dir),
        ("relationships", self.relationships_cache_dir)
    ]:
        cache_files = list(cache_dir.glob("*.json"))
        
        if cache_files:
            total_size = sum(f.stat().st_size for f in cache_files)
            avg_age = (time.time() - sum(f.stat().st_mtime for f in cache_files) / len(cache_files)) / 3600
            
            stats[cache_name] = {
                "file_count": len(cache_files),
                "total_size_mb": total_size / (1024 * 1024),
                "avg_age_hours": avg_age,
                "ttl_hours": CACHE_LAYERS[cache_name]["ttl"] / 3600
            }
        else:
            stats[cache_name] = {
                "file_count": 0,
                "total_size_mb": 0,
                "avg_age_hours": 0,
                "ttl_hours": CACHE_LAYERS[cache_name]["ttl"] / 3600
            }
    
    return stats

def print_cache_summary(self):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã®è¡¨ç¤º"""
    stats = self.get_cache_statistics()
    
    print("\nğŸ“Š ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ:")
    for cache_name, data in stats.items():
        print(f"  {cache_name.upper()}: {data['file_count']}ä»¶, "
              f"{data['total_size_mb']:.1f}MB, "
              f"å¹³å‡{data['avg_age_hours']:.1f}æ™‚é–“çµŒé")
```

## æœ€é©åŒ–ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã®é¸æŠ
```python
def choose_cache_strategy(operation_type, data_freshness_requirement):
    """æ“ä½œã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥é¸æŠ"""
    
    strategies = {
        "bulk_processing": {
            "use_cache": True,
            "cache_layers": ["lookup", "profiles", "relationships"],
            "freshness_tolerance": "medium"
        },
        "real_time_check": {
            "use_cache": True,
            "cache_layers": ["lookup", "profiles"],
            "freshness_tolerance": "high"  # é–¢ä¿‚æƒ…å ±ã¯æœ€æ–°å¿…é ˆ
        },
        "batch_analysis": {
            "use_cache": True,
            "cache_layers": ["lookup", "profiles", "relationships"],
            "freshness_tolerance": "low"
        }
    }
    
    return strategies.get(operation_type, strategies["bulk_processing"])
```

### ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡æ¸¬å®š
```python
def measure_cache_efficiency(self):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ã®æ¸¬å®š"""
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡
    cache_hits = self.cache_hit_counter
    cache_misses = self.cache_miss_counter
    hit_rate = cache_hits / (cache_hits + cache_misses) if (cache_hits + cache_misses) > 0 else 0
    
    # APIå‘¼ã³å‡ºã—å‰Šæ¸›ç‡
    api_calls_saved = self.api_calls_saved_counter
    total_potential_calls = self.total_operations_counter
    api_reduction_rate = api_calls_saved / total_potential_calls if total_potential_calls > 0 else 0
    
    return {
        "cache_hit_rate": hit_rate,
        "api_reduction_rate": api_reduction_rate,
        "total_cache_hits": cache_hits,
        "total_cache_misses": cache_misses,
        "api_calls_saved": api_calls_saved
    }
```